{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import atexit\n",
    "import io\n",
    "import json\n",
    "import platform\n",
    "import sys\n",
    "import webbrowser # Kept for potential future use, though not used in this non-web version\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "# asyncio, aiohttp, and torch.multiprocessing.Queue are removed as they were for web interface\n",
    "from dataclasses import dataclass, is_dataclass\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from PIL import Image, ImageCms\n",
    "from tifffile import TIFF, TiffWriter\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.multiprocessing as mp # Kept for mp.set_start_method if needed, but not for WebInterface queue\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Global sRGB Profile ---\n",
    "from pathlib import Path\n",
    "\n",
    "# In Colab, files are typically uploaded to the root working directory (/content/).\n",
    "# So, we assume 'sRGB Profile.icc' will be directly accessible by its name.\n",
    "try:\n",
    "    srgb_profile = Path('sRGB Profile.icc').read_bytes()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'sRGB Profile.icc' not found. Please upload it to your Colab session.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- From sqrtm.py (Matrix Square Root functions) ---\n",
    "def sqrtm_ns(a, num_iters=10):\n",
    "    \"\"\"Newton-Schulz iteration for matrix square root.\"\"\"\n",
    "    if a.ndim < 2:\n",
    "        raise RuntimeError('tensor of matrices must have at least 2 dimensions')\n",
    "    if a.shape[-2] != a.shape[-1]:\n",
    "        raise RuntimeError('tensor must be batches of square matrices')\n",
    "    if num_iters < 0:\n",
    "        raise RuntimeError('num_iters must not be negative')\n",
    "    norm_a = a.pow(2).sum(dim=[-2, -1], keepdim=True).sqrt()\n",
    "    y = a / norm_a\n",
    "    eye = torch.eye(a.shape[-1], device=a.device, dtype=a.dtype) * 3\n",
    "    z = torch.eye(a.shape[-1], device=a.device, dtype=a.dtype)\n",
    "    z = z.repeat([*a.shape[:-2], 1, 1])\n",
    "    for i in range(num_iters):\n",
    "        t = (eye - z @ y) / 2\n",
    "        y = y @ t\n",
    "        z = t @ z\n",
    "    return y * norm_a.sqrt()\n",
    "\n",
    "\n",
    "class _MatrixSquareRootNSLyap(torch.autograd.Function):\n",
    "    \"\"\"Autograd function for Newton-Schulz matrix square root (Lyapunov-based backward pass).\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, num_iters, num_iters_backward):\n",
    "        z = sqrtm_ns(a, num_iters)\n",
    "        ctx.save_for_backward(z, torch.tensor(num_iters_backward))\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z, num_iters = ctx.saved_tensors\n",
    "        norm_z = z.pow(2).sum(dim=[-2, -1], keepdim=True).sqrt()\n",
    "        a = z / norm_z\n",
    "        eye = torch.eye(z.shape[-1], device=z.device, dtype=z.dtype) * 3\n",
    "        q = grad_output / norm_z\n",
    "        for i in range(num_iters):\n",
    "            eye_a_a = eye - a @ a\n",
    "            q = q = (q @ eye_a_a - a.transpose(-2, -1) @ (a.transpose(-2, -1) @ q - q @ a)) / 2\n",
    "            if i < num_iters - 1:\n",
    "                a = a @ eye_a_a / 2\n",
    "        return q / 2, None, None\n",
    "\n",
    "\n",
    "def sqrtm_ns_lyap(a, num_iters=10, num_iters_backward=None):\n",
    "    \"\"\"Matrix square root using Newton-Schulz with Lyapunov-based backward pass.\"\"\"\n",
    "    if num_iters_backward is None:\n",
    "        num_iters_backward = num_iters\n",
    "    if num_iters_backward < 0:\n",
    "        raise RuntimeError('num_iters_backward must not be negative')\n",
    "    return _MatrixSquareRootNSLyap.apply(a, num_iters, num_iters_backward)\n",
    "\n",
    "\n",
    "class _MatrixSquareRootEig(torch.autograd.Function):\n",
    "    \"\"\"Autograd function for matrix square root using eigenvalue decomposition.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        vals, vecs = torch.linalg.eigh(a)\n",
    "        ctx.save_for_backward(vals, vecs)\n",
    "        return vecs @ vals.abs().sqrt().diag_embed() @ vecs.transpose(-2, -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        vals, vecs = ctx.saved_tensors\n",
    "        d = vals.abs().sqrt().unsqueeze(-1).repeat_interleave(vals.shape[-1], -1)\n",
    "        vecs_t = vecs.transpose(-2, -1)\n",
    "        return vecs @ (vecs_t @ grad_output @ vecs / (d + d.transpose(-2, -1))) @ vecs_t\n",
    "\n",
    "\n",
    "def sqrtm_eig(a):\n",
    "    \"\"\"Matrix square root using eigenvalue decomposition.\"\"\"\n",
    "    if a.ndim < 2:\n",
    "        raise RuntimeError('tensor of matrices must have at least 2 dimensions')\n",
    "    if a.shape[-2] != a.shape[-1]:\n",
    "        raise RuntimeError('tensor must be batches of square matrices')\n",
    "    return _MatrixSquareRootEig.apply(a)\n",
    "\n",
    "\n",
    "# --- From style_transfer.py (Core Style Transfer Logic) ---\n",
    "@dataclass\n",
    "class STIterate:\n",
    "    \"\"\"Dataclass to hold information about each style transfer iteration.\"\"\"\n",
    "    w: int\n",
    "    h: int\n",
    "    i: int\n",
    "    i_max: int\n",
    "    loss: float\n",
    "    time: float\n",
    "    gpu_ram: int\n",
    "\n",
    "\n",
    "class VGGFeatures(nn.Module):\n",
    "    \"\"\"VGG19 feature extractor for style transfer.\"\"\"\n",
    "    poolings = {'max': nn.MaxPool2d, 'average': nn.AvgPool2d, 'l2': partial(nn.LPPool2d, 2)}\n",
    "    pooling_scales = {'max': 1., 'average': 2., 'l2': 0.78}\n",
    "\n",
    "    def __init__(self, layers, pooling='max'):\n",
    "        super().__init__()\n",
    "        self.layers = sorted(set(layers))\n",
    "\n",
    "        # PyTorch pre-trained VGG-19 expects sRGB inputs in the range [0, 1]\n",
    "        # which are then normalized according to this transform.\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Load pre-trained VGG19 features\n",
    "        self.model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:self.layers[-1] + 1]\n",
    "        self.devices = [torch.device('cpu')] * len(self.model) # Default to CPU\n",
    "\n",
    "        # Reduces edge artifacts by changing padding mode of the first conv layer.\n",
    "        self.model[0] = self._change_padding_mode(self.model[0], 'replicate')\n",
    "\n",
    "        pool_scale = self.pooling_scales[pooling]\n",
    "        for i, layer in enumerate(self.model):\n",
    "            if pooling != 'max' and isinstance(layer, nn.MaxPool2d):\n",
    "                # Change pooling type and rescale activations if not max pooling.\n",
    "                self.model[i] = Scale(self.poolings[pooling](2), pool_scale)\n",
    "\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        self.model.requires_grad_(False) # Freeze model parameters\n",
    "\n",
    "    @staticmethod\n",
    "    def _change_padding_mode(conv, padding_mode):\n",
    "        \"\"\"Helper to change padding mode of a Conv2d layer.\"\"\"\n",
    "        new_conv = nn.Conv2d(conv.in_channels, conv.out_channels, conv.kernel_size,\n",
    "                             stride=conv.stride, padding=conv.padding,\n",
    "                             padding_mode=padding_mode)\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight.copy_(conv.weight)\n",
    "            new_conv.bias.copy_(conv.bias)\n",
    "        return new_conv\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_min_size(layers):\n",
    "        \"\"\"Calculates minimum input size required for the VGG layers.\"\"\"\n",
    "        last_layer = max(layers)\n",
    "        min_size = 1\n",
    "        for layer in [4, 9, 18, 27, 36]: # MaxPool2d layers in VGG19\n",
    "            if last_layer < layer:\n",
    "                break\n",
    "            min_size *= 2\n",
    "        return min_size\n",
    "\n",
    "    def distribute_layers(self, devices_map):\n",
    "        \"\"\"Distributes VGG layers across specified devices.\"\"\"\n",
    "        for i, layer in enumerate(self.model):\n",
    "            if i in devices_map:\n",
    "                device = torch.device(devices_map[i])\n",
    "            self.model[i] = layer.to(device)\n",
    "            self.devices[i] = device\n",
    "\n",
    "    def forward(self, input, layers=None):\n",
    "        \"\"\"Forward pass through VGG to extract features from specified layers.\"\"\"\n",
    "        layers = self.layers if layers is None else sorted(set(layers))\n",
    "        h, w = input.shape[2:4]\n",
    "        min_size = self._get_min_size(layers)\n",
    "        if min(h, w) < min_size:\n",
    "            raise ValueError(f'Input is {h}x{w} but must be at least {min_size}x{min_size}')\n",
    "        feats = {'input': input}\n",
    "        input = self.normalize(input)\n",
    "        for i in range(max(layers) + 1):\n",
    "            input = self.model[i](input.to(self.devices[i]))\n",
    "            if i in layers:\n",
    "                feats[i] = input\n",
    "        return feats\n",
    "\n",
    "\n",
    "class ScaledMSELoss(nn.Module):\n",
    "    \"\"\"Computes MSE scaled such that its gradient L1 norm is approximately 1.\"\"\"\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.register_buffer('eps', torch.tensor(eps))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'eps={self.eps:g}'\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        diff = input - target\n",
    "        return diff.pow(2).sum() / diff.abs().sum().add(self.eps)\n",
    "\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \"\"\"Content loss module (uses ScaledMSELoss).\"\"\"\n",
    "    def __init__(self, target, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.register_buffer('target', target)\n",
    "        self.loss = ScaledMSELoss(eps=eps)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.loss(input, self.target)\n",
    "\n",
    "\n",
    "class ContentLossMSE(nn.Module):\n",
    "    \"\"\"Content loss module (uses standard MSELoss).\"\"\"\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.register_buffer('target', target)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.loss(input, self.target)\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    \"\"\"Style loss module using Gram matrix and ScaledMSELoss.\"\"\"\n",
    "    def __init__(self, target, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.register_buffer('target', target)\n",
    "        self.loss = ScaledMSELoss(eps=eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target(target):\n",
    "        \"\"\"Computes the Gram matrix for style representation.\"\"\"\n",
    "        mat = target.flatten(-2)\n",
    "        # The Gram matrix normalization differs from Gatys et al. (2015) and Johnson et al.\n",
    "        return mat @ mat.transpose(-2, -1) / mat.shape[-1]\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.loss(self.get_target(input), self.target)\n",
    "\n",
    "\n",
    "def eye_like(x):\n",
    "    \"\"\"Returns an identity matrix with the same shape and device as x.\"\"\"\n",
    "    return torch.eye(x.shape[-2], x.shape[-1], dtype=x.dtype, device=x.device).expand_as(x)\n",
    "\n",
    "\n",
    "class StyleLossW2(nn.Module):\n",
    "    \"\"\"Wasserstein-2 style loss.\"\"\"\n",
    "    def __init__(self, target, eps=1e-4):\n",
    "        super().__init__()\n",
    "        # Use sqrtm_ns_lyap from sqrtm.py\n",
    "        self.sqrtm = partial(sqrtm_ns_lyap, num_iters=12)\n",
    "        mean, srm = target\n",
    "        cov = self.srm_to_cov(mean, srm) + eye_like(srm) * eps\n",
    "        self.register_buffer('mean', mean)\n",
    "        self.register_buffer('cov', cov)\n",
    "        self.register_buffer('cov_sqrt', self.sqrtm(cov))\n",
    "        self.register_buffer('eps', mean.new_tensor(eps))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target(target):\n",
    "        \"\"\"Compute the mean and second raw moment of the target activations.\"\"\"\n",
    "        mean = target.mean([-2, -1])\n",
    "        srm = torch.einsum('...chw,...dhw->...cd', target, target) / (target.shape[-2] * target.shape[-1])\n",
    "        return mean, srm\n",
    "\n",
    "    @staticmethod\n",
    "    def srm_to_cov(mean, srm):\n",
    "        \"\"\"Compute the covariance matrix from the mean and second raw moment.\"\"\"\n",
    "        return srm - torch.einsum('...c,...d->...cd', mean, mean)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mean, srm = self.get_target(input)\n",
    "        cov = self.srm_to_cov(mean, srm) + eye_like(srm) * self.eps\n",
    "        mean_diff = torch.mean((mean - self.mean) ** 2)\n",
    "        sqrt_term = self.sqrtm(self.cov_sqrt @ cov @ self.cov_sqrt)\n",
    "        cov_diff = torch.diagonal(self.cov + cov - 2 * sqrt_term, dim1=-2, dim2=-1).mean()\n",
    "        return mean_diff + cov_diff\n",
    "\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    \"\"\"L2 total variation loss (nine point stencil) for image smoothing.\"\"\"\n",
    "    def forward(self, input):\n",
    "        input = F.pad(input, (1, 1, 1, 1), 'replicate')\n",
    "        s1, s2 = slice(1, -1), slice(2, None)\n",
    "        s3, s4 = slice(None, -1), slice(1, None)\n",
    "        d1 = (input[..., s1, s2] - input[..., s1, s1]).pow(2).mean() / 3\n",
    "        d2 = (input[..., s2, s1] - input[..., s1, s1]).pow(2).mean() / 3\n",
    "        d3 = (input[..., s4, s4] - input[..., s3, s3]).pow(2).mean() / 12\n",
    "        d4 = (input[..., s4, s3] - input[..., s3, s4]).pow(2).mean() / 12\n",
    "        return 2 * (d1 + d2 + d3 + d4)\n",
    "\n",
    "\n",
    "class SumLoss(nn.ModuleList):\n",
    "    \"\"\"Combines multiple loss functions into a single sum.\"\"\"\n",
    "    def __init__(self, losses, verbose=False):\n",
    "        super().__init__(losses)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        losses = [loss(*args, **kwargs) for loss in self]\n",
    "        if self.verbose:\n",
    "            for i, loss in enumerate(losses):\n",
    "                print(f'({i}): {loss.item():g}')\n",
    "        return sum(loss.to(losses[-1].device) for loss in losses)\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    \"\"\"Applies a scaling factor to the output of a module.\"\"\"\n",
    "    def __init__(self, module, scale):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.register_buffer('scale', torch.tensor(scale))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'(scale): {self.scale.item():g}'\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.module(*args, **kwargs) * self.scale\n",
    "\n",
    "\n",
    "class LayerApply(nn.Module):\n",
    "    \"\"\"Applies a module to a specific layer's output from a dictionary of features.\"\"\"\n",
    "    def __init__(self, module, layer):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.layer = layer\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'(layer): {self.layer!r}'\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.module(input[self.layer])\n",
    "\n",
    "\n",
    "class EMA(nn.Module):\n",
    "    \"\"\"A bias-corrected exponential moving average, as in Kingma et al. (Adam).\"\"\"\n",
    "    def __init__(self, input, decay):\n",
    "        super().__init__()\n",
    "        self.register_buffer('value', torch.zeros_like(input))\n",
    "        self.register_buffer('decay', torch.tensor(decay))\n",
    "        self.register_buffer('accum', torch.tensor(1.))\n",
    "        self.update(input)\n",
    "\n",
    "    def get(self):\n",
    "        return self.value / (1 - self.accum)\n",
    "\n",
    "    def update(self, input):\n",
    "        self.accum *= self.decay\n",
    "        self.value *= self.decay\n",
    "        self.value += (1 - self.decay) * input\n",
    "\n",
    "\n",
    "def size_to_fit(size, max_dim, scale_up=False):\n",
    "    \"\"\"Calculates new dimensions to fit an image within max_dim while preserving aspect ratio.\"\"\"\n",
    "    w, h = size\n",
    "    if not scale_up and max(h, w) <= max_dim:\n",
    "        return w, h\n",
    "    new_w, new_h = max_dim, max_dim\n",
    "    if h > w:\n",
    "        new_w = round(max_dim * w / h)\n",
    "    else:\n",
    "        new_h = round(max_dim * h / w)\n",
    "    return new_w, new_h\n",
    "\n",
    "\n",
    "def gen_scales(start, end):\n",
    "    \"\"\"Generates a list of scales for multi-scale style transfer.\"\"\"\n",
    "    scale = end\n",
    "    i = 0\n",
    "    scales = set()\n",
    "    while scale >= start:\n",
    "        scales.add(scale)\n",
    "        i += 1\n",
    "        scale = round(end / pow(2, i/2))\n",
    "    return sorted(scales)\n",
    "\n",
    "\n",
    "def interpolate(*args, **kwargs):\n",
    "    \"\"\"Wrapper for F.interpolate to suppress warnings.\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', UserWarning)\n",
    "        return F.interpolate(*args, **kwargs)\n",
    "\n",
    "\n",
    "def scale_adam(state, shape):\n",
    "    \"\"\"Prepares an Adam optimizer state dict to warm-start at a new image scale.\"\"\"\n",
    "    state = copy.deepcopy(state)\n",
    "    for group in state['state'].values():\n",
    "        exp_avg, exp_avg_sq = group['exp_avg'], group['exp_avg_sq']\n",
    "        group['exp_avg'] = interpolate(exp_avg, shape, mode='bicubic')\n",
    "        group['exp_avg_sq'] = interpolate(exp_avg_sq, shape, mode='bilinear').relu_()\n",
    "        if 'max_exp_avg_sq' in group:\n",
    "            max_exp_avg_sq = group['max_exp_avg_sq']\n",
    "            group['max_exp_avg_sq'] = interpolate(max_exp_avg_sq, shape, mode='bilinear').relu_()\n",
    "    return state\n",
    "\n",
    "\n",
    "class StyleTransfer:\n",
    "    \"\"\"Main class for performing neural style transfer.\"\"\"\n",
    "    def __init__(self, devices=['cpu'], pooling='max'):\n",
    "        self.devices = [torch.device(device) for device in devices]\n",
    "        self.image = None\n",
    "        self.average = None\n",
    "\n",
    "        # Default content and style layers follow Gatys et al. (2015).\n",
    "        self.content_layers = [22]\n",
    "        self.style_layers = [1, 6, 11, 20, 29]\n",
    "\n",
    "        # The weighting of the style layers differs from Gatys et al. (2015) and Johnson et al.\n",
    "        style_weights = [256, 64, 16, 4, 1]\n",
    "        weight_sum = sum(abs(w) for w in style_weights)\n",
    "        self.style_weights = [w / weight_sum for w in style_weights]\n",
    "\n",
    "        self.model = VGGFeatures(self.style_layers + self.content_layers, pooling=pooling)\n",
    "\n",
    "        # Device distribution plan for VGG layers\n",
    "        if len(self.devices) == 1:\n",
    "            device_plan = {0: self.devices[0]}\n",
    "        elif len(self.devices) == 2:\n",
    "            device_plan = {0: self.devices[0], 5: self.devices[1]} # Distribute across two GPUs\n",
    "        else:\n",
    "            raise ValueError('Only 1 or 2 devices are supported for VGG layer distribution.')\n",
    "\n",
    "        self.model.distribute_layers(device_plan)\n",
    "\n",
    "    def get_image_tensor(self):\n",
    "        \"\"\"Returns the current averaged generated image as a detached tensor.\"\"\"\n",
    "        if self.average is not None:\n",
    "            return self.average.get().detach()[0].clamp(0, 1)\n",
    "        return None\n",
    "\n",
    "    def get_image(self, image_type='pil'):\n",
    "        \"\"\"Returns the generated image in the specified format.\"\"\"\n",
    "        image_tensor = self.get_image_tensor()\n",
    "        if image_tensor is not None:\n",
    "            if image_type.lower() == 'pil':\n",
    "                return TF.to_pil_image(image_tensor)\n",
    "            elif image_type.lower() == 'np_uint16':\n",
    "                arr = image_tensor.cpu().movedim(0, 2).numpy()\n",
    "                return np.uint16(np.round(arr * 65535))\n",
    "            else:\n",
    "                raise ValueError(\"image_type must be 'pil' or 'np_uint16'\")\n",
    "        return None\n",
    "\n",
    "    def stylize(self, content_image, style_images, *,\n",
    "                style_weights=None,\n",
    "                content_weight: float = 0.015,\n",
    "                tv_weight: float = 2.,\n",
    "                optimizer: str = 'adam',\n",
    "                min_scale: int = 128,\n",
    "                end_scale: int = 512,\n",
    "                iterations: int = 500,\n",
    "                initial_iterations: int = 1000,\n",
    "                step_size: float = 0.02,\n",
    "                avg_decay: float = 0.99,\n",
    "                init: str = 'content',\n",
    "                style_scale_fac: float = 1.,\n",
    "                style_size: int = None,\n",
    "                callback=None):\n",
    "        \"\"\"\n",
    "        Performs the neural style transfer process.\n",
    "\n",
    "        Args:\n",
    "            content_image (PIL.Image): The content image.\n",
    "            style_images (list of PIL.Image): List of style images.\n",
    "            style_weights (list of float, optional): Relative weights for each style image.\n",
    "            content_weight (float): Weight for the content loss.\n",
    "            tv_weight (float): Weight for the total variation (smoothing) loss.\n",
    "            optimizer (str): Optimizer to use ('adam' or 'lbfgs').\n",
    "            min_scale (int): Minimum image dimension during multi-scale optimization.\n",
    "            end_scale (int): Final image dimension for optimization.\n",
    "            iterations (int): Number of iterations per scale (after initial scale).\n",
    "            initial_iterations (int): Number of iterations for the first (smallest) scale.\n",
    "            step_size (float): Learning rate for Adam optimizer.\n",
    "            avg_decay (float): EMA decay rate for iterate averaging.\n",
    "            init (str): Initialization method for the generated image ('content', 'gray', 'uniform', 'normal', 'style_stats').\n",
    "            style_scale_fac (float): Relative scale of the style image to the content image.\n",
    "            style_size (int, optional): Fixed scale for style images, overrides style_scale_fac if set.\n",
    "            callback (callable, optional): A function to call after each iteration.\n",
    "        \"\"\"\n",
    "        min_scale = min(min_scale, end_scale)\n",
    "        content_weights = [content_weight / len(self.content_layers)] * len(self.content_layers)\n",
    "\n",
    "        if style_weights is None:\n",
    "            style_weights = [1 / len(style_images)] * len(style_images)\n",
    "        else:\n",
    "            weight_sum = sum(abs(w) for w in style_weights)\n",
    "            style_weights = [weight / weight_sum for weight in style_weights]\n",
    "        if len(style_images) != len(style_weights):\n",
    "            raise ValueError('style_images and style_weights must have the same length')\n",
    "\n",
    "        tv_loss = Scale(LayerApply(TVLoss(), 'input'), tv_weight)\n",
    "\n",
    "        scales = gen_scales(min_scale, end_scale)\n",
    "\n",
    "        # Initialize the generated image based on the chosen method\n",
    "        cw, ch = size_to_fit(content_image.size, scales[0], scale_up=True)\n",
    "        if init == 'content':\n",
    "            self.image = TF.to_tensor(content_image.resize((cw, ch), Image.BICUBIC))[None]\n",
    "        elif init == 'gray':\n",
    "            self.image = torch.rand([1, 3, ch, cw]) / 255 + 0.5 # A gray-ish random image\n",
    "        elif init == 'uniform':\n",
    "            self.image = torch.rand([1, 3, ch, cw]) # Uniform random image\n",
    "        elif init == 'normal':\n",
    "            self.image = torch.empty([1, 3, ch, cw])\n",
    "            nn.init.trunc_normal_(self.image, mean=0.5, std=0.25, a=0, b=1) # Truncated normal\n",
    "        elif init == 'style_stats':\n",
    "            # Initialize based on mean and variance of style images\n",
    "            means, variances = [], []\n",
    "            for i, image in enumerate(style_images):\n",
    "                my_image = TF.to_tensor(image)\n",
    "                means.append(my_image.mean(dim=(1, 2)) * style_weights[i])\n",
    "                variances.append(my_image.var(dim=(1, 2)) * style_weights[i])\n",
    "            means = sum(means)\n",
    "            variances = sum(variances)\n",
    "            channels = []\n",
    "            for mean, variance in zip(means, variances):\n",
    "                channel = torch.empty([1, 1, ch, cw])\n",
    "                nn.init.trunc_normal_(channel, mean=mean, std=variance.sqrt(), a=0, b=1)\n",
    "                channels.append(channel)\n",
    "            self.image = torch.cat(channels, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"init must be one of 'content', 'gray', 'uniform', 'normal', 'style_stats'\")\n",
    "        self.image = self.image.to(self.devices[0])\n",
    "\n",
    "        opt = None # Optimizer will be initialized per scale\n",
    "\n",
    "        # Multi-scale optimization loop\n",
    "        for scale in scales:\n",
    "            if self.devices[0].type == 'cuda':\n",
    "                torch.cuda.empty_cache() # Clear CUDA cache for memory efficiency\n",
    "\n",
    "            cw, ch = size_to_fit(content_image.size, scale, scale_up=True)\n",
    "            content = TF.to_tensor(content_image.resize((cw, ch), Image.BICUBIC))[None]\n",
    "            content = content.to(self.devices[0])\n",
    "\n",
    "            # Resize the generated image to the current scale\n",
    "            self.image = interpolate(self.image.detach(), (ch, cw), mode='bicubic').clamp(0, 1)\n",
    "            self.average = EMA(self.image, avg_decay) # Initialize Exponential Moving Average\n",
    "            self.image.requires_grad_() # Enable gradient computation for the image\n",
    "\n",
    "            print(f'Processing content image ({cw}x{ch})...')\n",
    "            content_feats = self.model(content, layers=self.content_layers)\n",
    "            content_losses = []\n",
    "            for layer, weight in zip(self.content_layers, content_weights):\n",
    "                target = content_feats[layer]\n",
    "                content_losses.append(Scale(LayerApply(ContentLossMSE(target), layer), weight))\n",
    "\n",
    "            style_targets, style_losses = {}, []\n",
    "            for i, image in enumerate(style_images):\n",
    "                # Determine style image size\n",
    "                if style_size is None:\n",
    "                    sw, sh = size_to_fit(image.size, round(scale * style_scale_fac))\n",
    "                else:\n",
    "                    sw, sh = size_to_fit(image.size, style_size)\n",
    "                style = TF.to_tensor(image.resize((sw, sh), Image.BICUBIC))[None]\n",
    "                style = style.to(self.devices[0])\n",
    "                print(f'Processing style image ({sw}x{sh})...')\n",
    "                style_feats = self.model(style, layers=self.style_layers)\n",
    "                # Aggregate style targets (means and covariance matrices) for multiple styles\n",
    "                for layer in self.style_layers:\n",
    "                    target_mean, target_cov = StyleLossW2.get_target(style_feats[layer])\n",
    "                    target_mean *= style_weights[i]\n",
    "                    target_cov *= style_weights[i]\n",
    "                    if layer not in style_targets:\n",
    "                        style_targets[layer] = target_mean, target_cov\n",
    "                    else:\n",
    "                        style_targets[layer][0].add_(target_mean)\n",
    "                        style_targets[layer][1].add_(target_cov)\n",
    "\n",
    "            for layer, weight in zip(self.style_layers, self.style_weights):\n",
    "                target = style_targets[layer]\n",
    "                style_losses.append(Scale(LayerApply(StyleLossW2(target), layer), weight))\n",
    "\n",
    "            crit = SumLoss([*content_losses, *style_losses, tv_loss]) # Combined loss function\n",
    "\n",
    "            # Optimizer setup\n",
    "            if optimizer == 'adam':\n",
    "                opt2 = optim.Adam([self.image], lr=step_size, betas=(0.9, 0.99))\n",
    "                # Warm-start Adam if not the first scale\n",
    "                if scale != scales[0] and opt is not None:\n",
    "                    opt_state = scale_adam(opt.state_dict(), (ch, cw))\n",
    "                    opt2.load_state_dict(opt_state)\n",
    "                opt = opt2\n",
    "            elif optimizer == 'lbfgs':\n",
    "                opt = optim.LBFGS([self.image], max_iter=1, history_size=10)\n",
    "            else:\n",
    "                raise ValueError(\"optimizer must be one of 'adam', 'lbfgs'\")\n",
    "\n",
    "            if self.devices[0].type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            def closure():\n",
    "                \"\"\"Closure for L-BFGS optimizer.\"\"\"\n",
    "                feats = self.model(self.image)\n",
    "                loss = crit(feats)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            actual_its = initial_iterations if scale == scales[0] else iterations\n",
    "            for i in range(1, actual_its + 1):\n",
    "                opt.zero_grad()\n",
    "                loss = opt.step(closure) # Perform optimization step\n",
    "                # Enforce box constraints [0, 1] for pixel values\n",
    "                if optimizer != 'lbfgs': # L-BFGS handles constraints differently\n",
    "                    with torch.no_grad():\n",
    "                        self.image.clamp_(0, 1)\n",
    "                self.average.update(self.image) # Update EMA of the image\n",
    "\n",
    "                # Callback for progress reporting and saving intermediate images\n",
    "                if callback is not None:\n",
    "                    gpu_ram = 0\n",
    "                    for device in self.devices:\n",
    "                        if device.type == 'cuda':\n",
    "                            gpu_ram = max(gpu_ram, torch.cuda.max_memory_allocated(device))\n",
    "                    callback(STIterate(w=cw, h=ch, i=i, i_max=actual_its, loss=loss.item(),\n",
    "                                       time=time.time(), gpu_ram=gpu_ram))\n",
    "\n",
    "            # Initialize next scale with the previous scale's averaged iterate.\n",
    "            with torch.no_grad():\n",
    "                self.image.copy_(self.average.get())\n",
    "\n",
    "        return self.get_image()\n",
    "\n",
    "\n",
    "# --- CLI-related functions (adapted for Colab) ---\n",
    "def prof_to_prof(image, src_prof, dst_prof, **kwargs):\n",
    "    \"\"\"Converts an image between ICC color profiles.\"\"\"\n",
    "    src_prof = io.BytesIO(src_prof)\n",
    "    dst_prof = io.BytesIO(dst_prof)\n",
    "    return ImageCms.profileToProfile(image, src_prof, dst_prof, **kwargs)\n",
    "\n",
    "\n",
    "def load_image(path, proof_prof=None):\n",
    "    \"\"\"Loads an image, handling ICC profiles and converting to RGB.\"\"\"\n",
    "    src_prof = dst_prof = srgb_profile\n",
    "    try:\n",
    "        image = Image.open(path)\n",
    "        if 'icc_profile' in image.info:\n",
    "            src_prof = image.info['icc_profile']\n",
    "        else:\n",
    "            image = image.convert('RGB') # Assume sRGB if no profile\n",
    "        if proof_prof is None:\n",
    "            if src_prof == dst_prof:\n",
    "                return image.convert('RGB')\n",
    "            # For Colab, proof_prof would also need to be a local file path\n",
    "            proof_prof_bytes = Path(proof_prof).read_bytes()\n",
    "            cmyk = prof_to_prof(image, src_prof, proof_prof_bytes, outputMode='CMYK')\n",
    "            return prof_to_prof(cmyk, proof_prof_bytes, dst_prof, outputMode='RGB')\n",
    "    except OSError as err:\n",
    "        print_error(err)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def save_pil(path, image):\n",
    "    \"\"\"Saves a PIL image to disk.\"\"\"\n",
    "    try:\n",
    "        kwargs = {'icc_profile': srgb_profile}\n",
    "        if path.suffix.lower() in {'.jpg', '.jpeg'}:\n",
    "            kwargs['quality'] = 95\n",
    "            kwargs['subsampling'] = 0\n",
    "        elif path.suffix.lower() == '.webp':\n",
    "            kwargs['quality'] = 95\n",
    "        image.save(path, **kwargs)\n",
    "    except (OSError, ValueError) as err:\n",
    "        print_error(err)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def save_tiff(path, image):\n",
    "    \"\"\"Saves a NumPy array image as TIFF with sRGB profile.\"\"\"\n",
    "    tag = ('InterColorProfile', TIFF.DATATYPES.BYTE, len(srgb_profile), srgb_profile, False)\n",
    "    try:\n",
    "        with TiffWriter(path) as writer:\n",
    "            writer.save(image, photometric='rgb', resolution=(72, 72), extratags=[tag])\n",
    "    except OSError as err:\n",
    "        print_error(err)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def save_image(path, image):\n",
    "    \"\"\"Saves an image (PIL or NumPy) to the specified path.\"\"\"\n",
    "    path = Path(path)\n",
    "    tqdm.write(f'Writing image to {path}.')\n",
    "    if isinstance(image, Image.Image):\n",
    "        save_pil(path, image)\n",
    "    elif isinstance(image, np.ndarray) and path.suffix.lower() in {'.tif', '.tiff'}:\n",
    "        save_tiff(path, image)\n",
    "    else:\n",
    "        raise ValueError('Unsupported combination of image type and extension')\n",
    "\n",
    "\n",
    "def get_safe_scale(w, h, dim):\n",
    "    \"\"\"Computes a safe end_scale for content image given GPU memory constraints.\"\"\"\n",
    "    return int(pow(w / h if w > h else h / w, 1/2) * dim)\n",
    "\n",
    "\n",
    "def print_error(err):\n",
    "    \"\"\"Prints an error message to stderr in red.\"\"\"\n",
    "    print('\\033[31m{}:\\033[0m {}'.format(type(err).__name__, err), file=sys.stderr)\n",
    "\n",
    "\n",
    "class Callback:\n",
    "    \"\"\"Callback class to report style transfer progress and save intermediate images.\"\"\"\n",
    "    def __init__(self, st, args, image_type='pil'): # web_interface removed\n",
    "        self.st = st\n",
    "        self.args = args\n",
    "        self.image_type = image_type\n",
    "        # self.web_interface = web_interface # Removed\n",
    "        self.iterates = []\n",
    "        self.progress = None\n",
    "\n",
    "    def __call__(self, iterate):\n",
    "        \"\"\"Called after each iteration.\"\"\"\n",
    "        self.iterates.append(iterate.__dict__) # Store as dict for JSON serialization\n",
    "        if iterate.i == 1:\n",
    "            self.progress = tqdm(total=iterate.i_max, dynamic_ncols=True)\n",
    "        msg = 'Size: {}x{}, iteration: {}, loss: {:g}'\n",
    "        tqdm.write(msg.format(iterate.w, iterate.h, iterate.i, iterate.loss))\n",
    "        self.progress.update()\n",
    "        # if self.web_interface is not None: # Removed web interface calls\n",
    "        #     self.web_interface.put_iterate(iterate, self.st.get_image_tensor())\n",
    "        if iterate.i == iterate.i_max:\n",
    "            self.progress.close()\n",
    "            # Save final image only if it's the last scale\n",
    "            if max(iterate.w, iterate.h) == self.args.end_scale:\n",
    "                pass # No web_interface.put_done()\n",
    "        elif iterate.i % self.args.save_every == 0:\n",
    "            # Save intermediate image\n",
    "            save_image(self.args.output, self.st.get_image(self.image_type))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the progress bar.\"\"\"\n",
    "        if self.progress is not None:\n",
    "            self.progress.close()\n",
    "\n",
    "    def get_trace(self):\n",
    "        \"\"\"Returns the full trace of arguments and iteration data.\"\"\"\n",
    "        return {'args': self.args.__dict__, 'iterates': self.iterates}\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to parse arguments and run the neural style transfer.\"\"\"\n",
    "    # setup_exceptions() and fix_start_method() are often not needed in Colab\n",
    "    # as the environment is pre-configured.\n",
    "    # For robust multiprocessing, 'spawn' is generally recommended for PyTorch.\n",
    "    # Colab often defaults to 'spawn' or handles it.\n",
    "    # if platform.system() != 'Windows':\n",
    "    #     mp.set_start_method('spawn', force=True) # Already handled in __main__ block\n",
    "\n",
    "    p = argparse.ArgumentParser(description=__doc__,\n",
    "                                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "    # Helper to get default values and types from StyleTransfer.stylize method\n",
    "    def arg_info(arg):\n",
    "        defaults = StyleTransfer.stylize.__kwdefaults__\n",
    "        annotations = StyleTransfer.stylize.__annotations__\n",
    "        return {'default': defaults.get(arg), 'type': annotations.get(arg)}\n",
    "\n",
    "    p.add_argument('content', type=str, help='the content image file path')\n",
    "    p.add_argument('styles', type=str, nargs='+', metavar='style', help='the style image file path(s)')\n",
    "    p.add_argument('--output', '-o', type=str, default='out.png',\n",
    "                   help='the output image file path')\n",
    "    p.add_argument('--style-weights', '-sw', type=float, nargs='+', default=None,\n",
    "                   metavar='STYLE_WEIGHT', help='the relative weights for each style image')\n",
    "    p.add_argument('--devices', type=str, default=[], nargs='+',\n",
    "                   help='the device names to use (e.g., \"cuda:0\", \"cpu\"; omit for auto-detect)')\n",
    "    p.add_argument('--random-seed', '-r', type=int, default=0,\n",
    "                   help='the random seed for reproducibility')\n",
    "    p.add_argument('--content-weight', '-cw', **arg_info('content_weight'),\n",
    "                   help='the content weight')\n",
    "    p.add_argument('--tv-weight', '-tw', **arg_info('tv_weight'),\n",
    "                   help='the smoothing weight (total variation loss)')\n",
    "    p.add_argument('--optimizer', **arg_info('optimizer'),\n",
    "                   choices=['adam', 'lbfgs'],\n",
    "                   help='the optimizer to use')\n",
    "    p.add_argument('--min-scale', '-ms', **arg_info('min_scale'),\n",
    "                   help='the minimum scale (max image dim), in pixels, for multi-scale optimization')\n",
    "    p.add_argument('--end-scale', '-s', type=str, default='512',\n",
    "                   help='the final scale (max image dim), in pixels (can end with \"+\" for safe scale)')\n",
    "    p.add_argument('--iterations', '-i', **arg_info('iterations'),\n",
    "                   help='the number of iterations per scale (after initial scale)')\n",
    "    p.add_argument('--initial-iterations', '-ii', **arg_info('initial_iterations'),\n",
    "                   help='the number of iterations on the first (smallest) scale')\n",
    "    p.add_argument('--save-every', type=int, default=50,\n",
    "                   help='save the image every SAVE_EVERY iterations (per scale)')\n",
    "    p.add_argument('--step-size', '-ss', **arg_info('step_size'),\n",
    "                   help='the step size (learning rate) for Adam optimizer')\n",
    "    p.add_argument('--avg-decay', '-ad', **arg_info('avg_decay'),\n",
    "                   help='the EMA decay rate for iterate averaging')\n",
    "    p.add_argument('--init', **arg_info('init'),\n",
    "                   choices=['content', 'gray', 'uniform', 'normal', 'style_stats'],\n",
    "                   help='the initial image initialization method')\n",
    "    p.add_argument('--style-scale-fac', **arg_info('style_scale_fac'),\n",
    "                   help='the relative scale of the style image to the content image')\n",
    "    p.add_argument('--style-size', **arg_info('style_size'),\n",
    "                   help='the fixed scale of the style at different content scales (overrides style-scale-fac)')\n",
    "    p.add_argument('--pooling', type=str, default='max', choices=['max', 'average', 'l2'],\n",
    "                   help='the model\\'s pooling mode (VGG feature extractor)')\n",
    "    p.add_argument('--proof', type=str, default=None,\n",
    "                   help='the ICC color profile (CMYK) for soft proofing the content and styles (file path)')\n",
    "    # Removed --web, --host, --port, --browser arguments\n",
    "\n",
    "    args = p.parse_args()\n",
    "\n",
    "    # Load content and style images\n",
    "    content_img = load_image(args.content, args.proof)\n",
    "    style_imgs = [load_image(img, args.proof) for img in args.styles]\n",
    "\n",
    "    # Determine output image type based on extension\n",
    "    image_type = 'pil'\n",
    "    if Path(args.output).suffix.lower() in {'.tif', '.tiff'}:\n",
    "        image_type = 'np_uint16'\n",
    "\n",
    "    # Setup devices (CUDA if available, otherwise CPU)\n",
    "    devices = [torch.device(device) for device in args.devices]\n",
    "    if not devices:\n",
    "        devices = [torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')]\n",
    "    if len(set(device.type for device in devices)) != 1:\n",
    "        print_error(ValueError('Devices must all be the same type (e.g., all cuda or all cpu).'))\n",
    "        sys.exit(1)\n",
    "    # The original code supported 1 or 2 devices for VGG distribution.\n",
    "    # This check is kept for consistency with the model's distribute_layers logic.\n",
    "    if not 1 <= len(devices) <= 2:\n",
    "        print_error(ValueError('Only 1 or 2 devices are supported for VGG layer distribution.'))\n",
    "        sys.exit(1)\n",
    "    print('Using devices:', ' '.join(str(device) for device in devices))\n",
    "\n",
    "    if devices[0].type == 'cpu':\n",
    "        print('CPU threads:', torch.get_num_threads())\n",
    "    if devices[0].type == 'cuda':\n",
    "        for i, device in enumerate(devices):\n",
    "            props = torch.cuda.get_device_properties(device)\n",
    "            print(f'GPU {i} type: {props.name} (compute {props.major}.{props.minor})')\n",
    "            print(f'GPU {i} RAM:', round(props.total_memory / 1024 / 1024), 'MB')\n",
    "\n",
    "    # Handle '+' in end_scale for safe scaling\n",
    "    end_scale_str = str(args.end_scale).rstrip('+')\n",
    "    try:\n",
    "        end_scale = int(end_scale_str)\n",
    "    except ValueError:\n",
    "        print_error(ValueError(f\"Invalid value for --end-scale: '{args.end_scale}'. Must be an integer or integer followed by '+'.\"))\n",
    "        sys.exit(1)\n",
    "\n",
    "    if str(args.end_scale).endswith('+'):\n",
    "        end_scale = get_safe_scale(*content_img.size, end_scale)\n",
    "    args.end_scale = end_scale # Update args with resolved end_scale\n",
    "\n",
    "    # web_interface is removed, so this block is no longer needed\n",
    "    # web_interface = None\n",
    "    # if args.web:\n",
    "    #     web_interface = WebInterface(args.host, args.port)\n",
    "    #     atexit.register(web_interface.close)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    for device in devices:\n",
    "        torch.tensor(0).to(device)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed) # Also set numpy seed for consistency if used\n",
    "\n",
    "    print('Loading model...')\n",
    "    st = StyleTransfer(devices=devices, pooling=args.pooling)\n",
    "    # Callback no longer takes web_interface argument\n",
    "    callback = Callback(st, args, image_type=image_type)\n",
    "    atexit.register(callback.close) # Ensure callback resources are closed on exit\n",
    "\n",
    "    # Browser opening logic removed as web interface is gone\n",
    "    # url = f'http://{args.host}:{args.port}/'\n",
    "    # if args.web:\n",
    "    #     if args.browser:\n",
    "    #         try:\n",
    "    #             webbrowser.get(args.browser).open(url)\n",
    "    #         except webbrowser.Error as e:\n",
    "    #             print_error(f\"Could not open browser '{args.browser}': {e}\")\n",
    "    #     elif args.browser is None:\n",
    "    #         try:\n",
    "    #             webbrowser.open(url)\n",
    "    #         except webbrowser.Error as e:\n",
    "    #             print_error(f\"Could not open default browser: {e}\")\n",
    "\n",
    "    # Prepare arguments for stylize method\n",
    "    defaults = StyleTransfer.stylize.__kwdefaults__\n",
    "    st_kwargs = {k: getattr(args, k) for k in defaults if hasattr(args, k)}\n",
    "\n",
    "    # Perform style transfer\n",
    "    try:\n",
    "        st.stylize(content_img, style_imgs, **st_kwargs, callback=callback)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStyle transfer interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print_error(e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Save final output image\n",
    "    output_image = st.get_image(image_type)\n",
    "    if output_image is not None:\n",
    "        save_image(args.output, output_image)\n",
    "        # Display the image in Colab after saving\n",
    "        try:\n",
    "            from IPython.display import Image as DisplayImage, display\n",
    "            display(DisplayImage(filename=str(args.output)))\n",
    "            print(f\"Generated image displayed and saved as '{args.output}'\")\n",
    "        except ImportError:\n",
    "            print(f\"Generated image saved as '{args.output}'\")\n",
    "    else:\n",
    "        print_error(RuntimeError(\"No output image generated.\"))\n",
    "\n",
    "    # Save trace data to JSON\n",
    "    try:\n",
    "        with open('trace.json', 'w') as fp:\n",
    "            json.dump(callback.get_trace(), fp, indent=4)\n",
    "        print(\"Trace data saved to 'trace.json'.\")\n",
    "    except Exception as e:\n",
    "        print_error(f\"Error saving trace data: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure multiprocessing context is set up correctly for all OS\n",
    "    # This is crucial when using multiprocessing.Process, especially with PyTorch.\n",
    "    # Colab environments typically handle this well, but 'spawn' is a safe default.\n",
    "    if platform.system() != 'Windows': # 'spawn' is default on Windows, but not on Unix/macOS\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
